; Environments
%include ./common-env.drake

;;
; Methods
;;

file() [eval]
  echo "$CODE" | tee $OUTPUT | nl

scaldrb()
  source $INPUT
  if [ "$IS_LOCAL" == "true" ]; then
    echo $[SCALDRB_EXEC] --local $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
    $[SCALDRB_EXEC] --local $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
  else
    echo $[SCALDRB_EXEC] $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
    $[SCALDRB_EXEC] $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
  fi

hadoop_jar()
  source $INPUT
  echo $[HADOOP_EXEC] jar $STANDALONE_JAR $JOB_CLASS --hdfs --input $ARG_INPUT --output $ARG_OUTPUT
  $[HADOOP_EXEC] jar $STANDALONE_JAR $JOB_CLASS --hdfs --input $ARG_INPUT --output $ARG_OUTPUT

;;
; Ad-hoc job with Scalding scald.rb script
;;

; import data
Import.scala <- [method:file method-mode:append]
  import com.twitter.scalding._
  class Import(args : Args) extends Job(args) {
    val textSource = TextLine(args("input"))
    val lines: TypedPipe[String] = TypedPipe.from[String](textSource) 
    lines.write(TypedTsv[String](args("output")))
  }

; local environment execution
import_local-env.sh <- Import.scala [method:file method-mode:append]
  SCALA_FILE=$[INPUT]
  ARG_INPUT=$[DATA_DIR_LOCAL]/excite/excite-small.log
  ARG_OUTPUT=../target/output
  IS_LOCAL=true

%import_local <- import_local-env.sh [method:scaldrb]

; cluster environment execution
import-env.sh <- Import.scala [method:file method-mode:append]
  SCALA_FILE=$[INPUT]
  ARG_INPUT=$[DATA_DIR_HDFS]/excite/excite-small.log
  ARG_OUTPUT=$[DATA_DIR_HDFS]/output/excite

%import <- import-env.sh [method:scaldrb]

;;
; Scalding standalone jar job with hadoop command
;;

STANDALONE_JAR=../target/scala-2.10/web-data-analytics-0.0.1.jar

; standalone jar is required. run 'sbt assembly'.
wordCountJob-env.sh <- $[STANDALONE_JAR] [method:file method-mode:append]
  STANDALONE_JAR=$[STANDALONE_JAR]
  JOB_CLASS=com.knownstylenolife.hadoop.scalding.WordCountJob
  ARG_INPUT=$[DATA_DIR_HDFS]/excite/excite-small.log
  ARG_OUTPUT=$[DATA_DIR_HDFS]/output/wordcount

%wordcount <- wordCountJob-env.sh [method:hadoop_jar]

