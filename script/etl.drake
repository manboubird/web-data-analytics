; Environments
%include common-env.drake

BASE=etl

;;
; Methods
;;

file() [eval]
  echo "$CODE" | tee $OUTPUT | nl

scaldrb()
  source $INPUT
  if [ "$IS_LOCAL" == "true" ]; then
    echo $[SCALDRB_EXEC] --local $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
    $[SCALDRB_EXEC] --local $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
  else
    echo $[SCALDRB_EXEC] $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
    $[SCALDRB_EXEC] $SCALA_FILE --input $ARG_INPUT --output $ARG_OUTPUT
  fi

;;
; Ad-hoc job with Scalding scald.rb script
;;

; import data
Import.scala <- [method:file method-mode:append]
  import com.twitter.scalding._
  class Import(args : Args) extends Job(args) {
    val textSource = TextLine(args("input"))
    val lines: TypedPipe[String] = TypedPipe.from[String](textSource) 
    lines.write(TypedTsv[String](args("output")))
  }

; local environment execution
import_local-env.sh <- Import.scala [method:file method-mode:append]
  SCALA_FILE=$[INPUT]
  ARG_INPUT=$[DATA_DIR_LOCAL]/excite/excite-small.log
  ARG_OUTPUT=../target/output
  IS_LOCAL=true

%import_local <- import_local-env.sh [method:scaldrb]

; cluster environment execution
import-env.sh <- Import.scala [method:file method-mode:append]
  SCALA_FILE=$[INPUT]
  ARG_INPUT=$[DATA_DIR_HDFS]/excite/excite-small.log
  ARG_OUTPUT=$[DATA_DIR_HDFS]/output/excite

%import <- import-env.sh [method:scaldrb]

; standalone jar is required. run 'sbt assembly'.
STANDALONE_JAR=../target/scala-2.10/web-data-analytics-0.0.1.jar

%AccessLogEtlJob_local <- !$[STANDALONE_JAR]
  $[HADOOP_EXEC] jar $[STANDALONE_JAR] com.knownstylenolife.hadoop.scalding.etl.AccessLogEtlJob --local --inDir $[DATA_DIR_LOCAL]/splunk/www1/access.log --outDir $[DATA_DIR_LOCAL]/output/AccessLogEtlJob/out --badDir $[DATA_DIR_LOCAL]/output/AccessLogEtlJob/bad --exceptionDir $[DATA_DIR_LOCAL]/output/AccessLogEtlJob/exception

